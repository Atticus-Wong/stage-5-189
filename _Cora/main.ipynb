{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4142ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Dataset_Loader_Node_Classification import Dataset_Loader\n",
    "from gcn.models import GCN\n",
    "from gcn.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e77221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Data Loaded:\n",
      "  Features shape: torch.Size([2708, 1433])\n",
      "  Adjacency matrix shape: torch.Size([2708, 2708])\n",
      "  Labels shape: torch.Size([2708])\n",
      "  Number of training samples: 140\n",
      "  Number of validation samples: 300\n",
      "  Number of testing samples: 1000\n"
     ]
    }
   ],
   "source": [
    "data_loader = Dataset_Loader(dName='cora', dDescription='Pubmed citation network')\n",
    "data_loader.dataset_name = 'cora'\n",
    "data_loader.dataset_source_folder_path = '../stage_5_data/cora/'\n",
    "\n",
    "loaded_data = data_loader.load()\n",
    "\n",
    "graph_data = loaded_data['graph']\n",
    "train_test_val_indices = loaded_data['train_test_val']\n",
    "\n",
    "adj = graph_data['utility']['A']\n",
    "features = graph_data['X']\n",
    "labels = graph_data['y']\n",
    "\n",
    "idx_train = train_test_val_indices['idx_train']\n",
    "idx_val = train_test_val_indices['idx_val']\n",
    "idx_test = train_test_val_indices['idx_test']\n",
    "\n",
    "print(\"Data Loaded:\")\n",
    "print(f\"  Features shape: {features.shape}\")\n",
    "print(f\"  Adjacency matrix shape: {adj.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print(f\"  Number of training samples: {len(idx_train)}\")\n",
    "print(f\"  Number of validation samples: {len(idx_val)}\")\n",
    "print(f\"  Number of testing samples: {len(idx_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb5b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized:\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (1433 -> 16)\n",
      "  (gc2): GraphConvolution (16 -> 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "hidden_units = 16\n",
    "dropout_rate = 0.5\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "n_features = features.shape[1]\n",
    "n_classes = labels.max().item() + 1\n",
    "\n",
    "model = GCN(nfeat=n_features,\n",
    "            nhid=hidden_units,\n",
    "            nclass=n_classes,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda_available:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "print(\"Model Initialized:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494318f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch: 0001 loss_train: 2.0091 acc_train: 0.1214 loss_val: 1.9381 acc_val: 0.2133 time: 0.1037s\n",
      "Epoch: 0002 loss_train: 1.8785 acc_train: 0.2643 loss_val: 1.8912 acc_val: 0.2867 time: 0.0051s\n",
      "Epoch: 0003 loss_train: 1.7898 acc_train: 0.3857 loss_val: 1.8391 acc_val: 0.3500 time: 0.0050s\n",
      "Epoch: 0004 loss_train: 1.6906 acc_train: 0.5000 loss_val: 1.7834 acc_val: 0.3367 time: 0.0056s\n",
      "Epoch: 0005 loss_train: 1.5828 acc_train: 0.5571 loss_val: 1.7247 acc_val: 0.3567 time: 0.0052s\n",
      "Epoch: 0006 loss_train: 1.4722 acc_train: 0.5786 loss_val: 1.6658 acc_val: 0.3567 time: 0.0044s\n",
      "Epoch: 0007 loss_train: 1.3349 acc_train: 0.6571 loss_val: 1.6071 acc_val: 0.3800 time: 0.0051s\n",
      "Epoch: 0008 loss_train: 1.2346 acc_train: 0.6071 loss_val: 1.5486 acc_val: 0.3867 time: 0.0046s\n",
      "Epoch: 0009 loss_train: 1.1384 acc_train: 0.6357 loss_val: 1.4913 acc_val: 0.4100 time: 0.0048s\n",
      "Epoch: 0010 loss_train: 1.0739 acc_train: 0.6357 loss_val: 1.4332 acc_val: 0.4300 time: 0.0053s\n",
      "Epoch: 0011 loss_train: 0.9484 acc_train: 0.7357 loss_val: 1.3738 acc_val: 0.4767 time: 0.0056s\n",
      "Epoch: 0012 loss_train: 0.9264 acc_train: 0.7500 loss_val: 1.3165 acc_val: 0.5233 time: 0.0051s\n",
      "Epoch: 0013 loss_train: 0.8097 acc_train: 0.8214 loss_val: 1.2624 acc_val: 0.5933 time: 0.0044s\n",
      "Epoch: 0014 loss_train: 0.7357 acc_train: 0.8143 loss_val: 1.2137 acc_val: 0.6400 time: 0.0045s\n",
      "Epoch: 0015 loss_train: 0.6521 acc_train: 0.8357 loss_val: 1.1692 acc_val: 0.6700 time: 0.0045s\n",
      "Epoch: 0016 loss_train: 0.6309 acc_train: 0.8643 loss_val: 1.1277 acc_val: 0.6967 time: 0.0045s\n",
      "Epoch: 0017 loss_train: 0.6119 acc_train: 0.8929 loss_val: 1.0891 acc_val: 0.7233 time: 0.0047s\n",
      "Epoch: 0018 loss_train: 0.5213 acc_train: 0.9214 loss_val: 1.0510 acc_val: 0.7333 time: 0.0046s\n",
      "Epoch: 0019 loss_train: 0.4999 acc_train: 0.9071 loss_val: 1.0113 acc_val: 0.7533 time: 0.0051s\n",
      "Epoch: 0020 loss_train: 0.4537 acc_train: 0.9143 loss_val: 0.9717 acc_val: 0.7767 time: 0.0046s\n",
      "Epoch: 0021 loss_train: 0.4057 acc_train: 0.9357 loss_val: 0.9362 acc_val: 0.7833 time: 0.0054s\n",
      "Epoch: 0022 loss_train: 0.3595 acc_train: 0.9500 loss_val: 0.9056 acc_val: 0.7833 time: 0.0085s\n",
      "Epoch: 0023 loss_train: 0.3747 acc_train: 0.9429 loss_val: 0.8764 acc_val: 0.7867 time: 0.0054s\n",
      "Epoch: 0024 loss_train: 0.3035 acc_train: 0.9571 loss_val: 0.8478 acc_val: 0.7933 time: 0.0063s\n",
      "Epoch: 0025 loss_train: 0.3272 acc_train: 0.9000 loss_val: 0.8222 acc_val: 0.8067 time: 0.0046s\n",
      "Epoch: 0026 loss_train: 0.2814 acc_train: 0.9357 loss_val: 0.7997 acc_val: 0.8000 time: 0.0048s\n",
      "Epoch: 0027 loss_train: 0.2660 acc_train: 0.9429 loss_val: 0.7817 acc_val: 0.8000 time: 0.0095s\n",
      "Epoch: 0028 loss_train: 0.2224 acc_train: 0.9643 loss_val: 0.7663 acc_val: 0.7967 time: 0.0088s\n",
      "Epoch: 0029 loss_train: 0.2205 acc_train: 0.9786 loss_val: 0.7556 acc_val: 0.8000 time: 0.0061s\n",
      "Epoch: 0030 loss_train: 0.2288 acc_train: 0.9643 loss_val: 0.7454 acc_val: 0.7967 time: 0.0057s\n",
      "Epoch: 0031 loss_train: 0.1662 acc_train: 0.9714 loss_val: 0.7368 acc_val: 0.7967 time: 0.0060s\n",
      "Epoch: 0032 loss_train: 0.1754 acc_train: 0.9571 loss_val: 0.7310 acc_val: 0.7900 time: 0.0054s\n",
      "Epoch: 0033 loss_train: 0.1507 acc_train: 0.9786 loss_val: 0.7273 acc_val: 0.7867 time: 0.0049s\n",
      "Epoch: 0034 loss_train: 0.1458 acc_train: 0.9857 loss_val: 0.7232 acc_val: 0.7867 time: 0.0054s\n",
      "Epoch: 0035 loss_train: 0.1357 acc_train: 0.9714 loss_val: 0.7207 acc_val: 0.7800 time: 0.0046s\n",
      "Epoch: 0036 loss_train: 0.1265 acc_train: 1.0000 loss_val: 0.7152 acc_val: 0.7800 time: 0.0049s\n",
      "Epoch: 0037 loss_train: 0.1149 acc_train: 0.9857 loss_val: 0.7100 acc_val: 0.7800 time: 0.0051s\n",
      "Epoch: 0038 loss_train: 0.1282 acc_train: 0.9857 loss_val: 0.7049 acc_val: 0.7833 time: 0.0048s\n",
      "Epoch: 0039 loss_train: 0.1254 acc_train: 0.9857 loss_val: 0.6997 acc_val: 0.7900 time: 0.0057s\n",
      "Epoch: 0040 loss_train: 0.1398 acc_train: 0.9786 loss_val: 0.6920 acc_val: 0.8033 time: 0.0047s\n",
      "Epoch: 0041 loss_train: 0.0877 acc_train: 0.9857 loss_val: 0.6855 acc_val: 0.8033 time: 0.0049s\n",
      "Epoch: 0042 loss_train: 0.1177 acc_train: 0.9714 loss_val: 0.6793 acc_val: 0.8000 time: 0.0049s\n",
      "Epoch: 0043 loss_train: 0.0828 acc_train: 0.9929 loss_val: 0.6737 acc_val: 0.8033 time: 0.0047s\n",
      "Epoch: 0044 loss_train: 0.0960 acc_train: 0.9714 loss_val: 0.6666 acc_val: 0.8100 time: 0.0050s\n",
      "Epoch: 0045 loss_train: 0.0960 acc_train: 0.9857 loss_val: 0.6610 acc_val: 0.8067 time: 0.0047s\n",
      "Epoch: 0046 loss_train: 0.1052 acc_train: 0.9786 loss_val: 0.6569 acc_val: 0.8133 time: 0.0048s\n",
      "Epoch: 0047 loss_train: 0.1152 acc_train: 0.9857 loss_val: 0.6531 acc_val: 0.8100 time: 0.0061s\n",
      "Epoch: 0048 loss_train: 0.0911 acc_train: 0.9857 loss_val: 0.6517 acc_val: 0.8100 time: 0.0046s\n",
      "Epoch: 0049 loss_train: 0.0730 acc_train: 0.9929 loss_val: 0.6531 acc_val: 0.7967 time: 0.0053s\n",
      "Epoch: 0050 loss_train: 0.0907 acc_train: 0.9786 loss_val: 0.6550 acc_val: 0.7967 time: 0.0048s\n",
      "Epoch: 0051 loss_train: 0.0798 acc_train: 0.9929 loss_val: 0.6552 acc_val: 0.8100 time: 0.0046s\n",
      "Epoch: 0052 loss_train: 0.0799 acc_train: 0.9857 loss_val: 0.6549 acc_val: 0.8067 time: 0.0045s\n",
      "Epoch: 0053 loss_train: 0.0902 acc_train: 0.9786 loss_val: 0.6545 acc_val: 0.8100 time: 0.0045s\n",
      "Epoch: 0054 loss_train: 0.0571 acc_train: 1.0000 loss_val: 0.6541 acc_val: 0.8067 time: 0.0050s\n",
      "Epoch: 0055 loss_train: 0.0745 acc_train: 0.9857 loss_val: 0.6537 acc_val: 0.8067 time: 0.0048s\n",
      "Epoch: 0056 loss_train: 0.0803 acc_train: 0.9786 loss_val: 0.6557 acc_val: 0.8033 time: 0.0049s\n",
      "Epoch: 0057 loss_train: 0.0434 acc_train: 1.0000 loss_val: 0.6580 acc_val: 0.8033 time: 0.0047s\n",
      "Epoch: 0058 loss_train: 0.0592 acc_train: 1.0000 loss_val: 0.6578 acc_val: 0.8033 time: 0.0051s\n",
      "Epoch: 0059 loss_train: 0.0455 acc_train: 0.9929 loss_val: 0.6578 acc_val: 0.8033 time: 0.0049s\n",
      "Epoch: 0060 loss_train: 0.0940 acc_train: 0.9714 loss_val: 0.6572 acc_val: 0.8033 time: 0.0043s\n",
      "Epoch: 0061 loss_train: 0.0772 acc_train: 0.9857 loss_val: 0.6559 acc_val: 0.8033 time: 0.0048s\n",
      "Epoch: 0062 loss_train: 0.0671 acc_train: 0.9857 loss_val: 0.6548 acc_val: 0.7967 time: 0.0058s\n",
      "Epoch: 0063 loss_train: 0.0704 acc_train: 0.9929 loss_val: 0.6529 acc_val: 0.7967 time: 0.0047s\n",
      "Epoch: 0064 loss_train: 0.0538 acc_train: 1.0000 loss_val: 0.6518 acc_val: 0.7933 time: 0.0052s\n",
      "Epoch: 0065 loss_train: 0.0582 acc_train: 1.0000 loss_val: 0.6514 acc_val: 0.7967 time: 0.0047s\n",
      "Epoch: 0066 loss_train: 0.0670 acc_train: 0.9929 loss_val: 0.6526 acc_val: 0.7967 time: 0.0049s\n",
      "Epoch: 0067 loss_train: 0.0677 acc_train: 0.9929 loss_val: 0.6542 acc_val: 0.7933 time: 0.0134s\n",
      "Epoch: 0068 loss_train: 0.0491 acc_train: 1.0000 loss_val: 0.6547 acc_val: 0.7867 time: 0.0086s\n",
      "Epoch: 0069 loss_train: 0.0602 acc_train: 1.0000 loss_val: 0.6535 acc_val: 0.7867 time: 0.0048s\n",
      "Epoch: 0070 loss_train: 0.0703 acc_train: 0.9857 loss_val: 0.6513 acc_val: 0.7867 time: 0.0055s\n",
      "Epoch: 0071 loss_train: 0.0556 acc_train: 0.9857 loss_val: 0.6491 acc_val: 0.7867 time: 0.0048s\n",
      "Epoch: 0072 loss_train: 0.0582 acc_train: 0.9929 loss_val: 0.6473 acc_val: 0.7900 time: 0.0045s\n",
      "Epoch: 0073 loss_train: 0.0582 acc_train: 1.0000 loss_val: 0.6464 acc_val: 0.7933 time: 0.0051s\n",
      "Epoch: 0074 loss_train: 0.0381 acc_train: 1.0000 loss_val: 0.6462 acc_val: 0.8000 time: 0.0048s\n",
      "Epoch: 0075 loss_train: 0.0538 acc_train: 0.9929 loss_val: 0.6438 acc_val: 0.7967 time: 0.0060s\n",
      "Epoch: 0076 loss_train: 0.0700 acc_train: 0.9857 loss_val: 0.6409 acc_val: 0.7933 time: 0.0061s\n",
      "Epoch: 0077 loss_train: 0.0540 acc_train: 0.9929 loss_val: 0.6389 acc_val: 0.7933 time: 0.0062s\n",
      "Epoch: 0078 loss_train: 0.0509 acc_train: 0.9929 loss_val: 0.6385 acc_val: 0.7967 time: 0.0045s\n",
      "Epoch: 0079 loss_train: 0.0438 acc_train: 1.0000 loss_val: 0.6371 acc_val: 0.8000 time: 0.0057s\n",
      "Epoch: 0080 loss_train: 0.0313 acc_train: 1.0000 loss_val: 0.6365 acc_val: 0.8000 time: 0.0044s\n",
      "Epoch: 0081 loss_train: 0.0453 acc_train: 1.0000 loss_val: 0.6361 acc_val: 0.8000 time: 0.0051s\n",
      "Epoch: 0082 loss_train: 0.0612 acc_train: 0.9714 loss_val: 0.6358 acc_val: 0.7967 time: 0.0047s\n",
      "Epoch: 0083 loss_train: 0.0518 acc_train: 0.9929 loss_val: 0.6351 acc_val: 0.7933 time: 0.0052s\n",
      "Epoch: 0084 loss_train: 0.0574 acc_train: 0.9929 loss_val: 0.6329 acc_val: 0.7933 time: 0.0045s\n",
      "Epoch: 0085 loss_train: 0.0985 acc_train: 0.9714 loss_val: 0.6305 acc_val: 0.7933 time: 0.0043s\n",
      "Epoch: 0086 loss_train: 0.0702 acc_train: 1.0000 loss_val: 0.6271 acc_val: 0.8000 time: 0.0048s\n",
      "Epoch: 0087 loss_train: 0.0473 acc_train: 0.9929 loss_val: 0.6245 acc_val: 0.8033 time: 0.0054s\n",
      "Epoch: 0088 loss_train: 0.0463 acc_train: 1.0000 loss_val: 0.6229 acc_val: 0.8033 time: 0.0046s\n",
      "Epoch: 0089 loss_train: 0.0326 acc_train: 1.0000 loss_val: 0.6227 acc_val: 0.8033 time: 0.0044s\n",
      "Epoch: 0090 loss_train: 0.0663 acc_train: 0.9929 loss_val: 0.6216 acc_val: 0.8033 time: 0.0049s\n",
      "Epoch: 0091 loss_train: 0.0606 acc_train: 1.0000 loss_val: 0.6190 acc_val: 0.8033 time: 0.0052s\n",
      "Epoch: 0092 loss_train: 0.0481 acc_train: 1.0000 loss_val: 0.6170 acc_val: 0.8033 time: 0.0046s\n",
      "Epoch: 0093 loss_train: 0.0456 acc_train: 1.0000 loss_val: 0.6139 acc_val: 0.8033 time: 0.0050s\n",
      "Epoch: 0094 loss_train: 0.0379 acc_train: 1.0000 loss_val: 0.6116 acc_val: 0.8067 time: 0.0045s\n",
      "Epoch: 0095 loss_train: 0.0495 acc_train: 0.9929 loss_val: 0.6110 acc_val: 0.8067 time: 0.0048s\n",
      "Epoch: 0096 loss_train: 0.0459 acc_train: 1.0000 loss_val: 0.6114 acc_val: 0.8100 time: 0.0053s\n",
      "Epoch: 0097 loss_train: 0.0362 acc_train: 1.0000 loss_val: 0.6131 acc_val: 0.8033 time: 0.0046s\n",
      "Epoch: 0098 loss_train: 0.0616 acc_train: 0.9786 loss_val: 0.6165 acc_val: 0.8000 time: 0.0057s\n",
      "Epoch: 0099 loss_train: 0.0529 acc_train: 0.9929 loss_val: 0.6207 acc_val: 0.7933 time: 0.0049s\n",
      "Epoch: 0100 loss_train: 0.0489 acc_train: 0.9929 loss_val: 0.6259 acc_val: 0.7900 time: 0.0046s\n",
      "Epoch: 0101 loss_train: 0.0529 acc_train: 1.0000 loss_val: 0.6266 acc_val: 0.7900 time: 0.0049s\n",
      "Epoch: 0102 loss_train: 0.0586 acc_train: 0.9857 loss_val: 0.6257 acc_val: 0.7833 time: 0.0045s\n",
      "Epoch: 0103 loss_train: 0.0746 acc_train: 0.9929 loss_val: 0.6239 acc_val: 0.7833 time: 0.0055s\n",
      "Epoch: 0104 loss_train: 0.0420 acc_train: 1.0000 loss_val: 0.6230 acc_val: 0.7900 time: 0.0049s\n",
      "Epoch: 0105 loss_train: 0.0462 acc_train: 0.9929 loss_val: 0.6233 acc_val: 0.7900 time: 0.0045s\n",
      "Epoch: 0106 loss_train: 0.0461 acc_train: 0.9929 loss_val: 0.6234 acc_val: 0.7933 time: 0.0045s\n",
      "Epoch: 0107 loss_train: 0.0523 acc_train: 1.0000 loss_val: 0.6206 acc_val: 0.7967 time: 0.0046s\n",
      "Epoch: 0108 loss_train: 0.0705 acc_train: 1.0000 loss_val: 0.6192 acc_val: 0.7967 time: 0.0048s\n",
      "Epoch: 0109 loss_train: 0.0542 acc_train: 0.9929 loss_val: 0.6154 acc_val: 0.7933 time: 0.0051s\n",
      "Epoch: 0110 loss_train: 0.0546 acc_train: 0.9929 loss_val: 0.6106 acc_val: 0.8033 time: 0.0049s\n",
      "Epoch: 0111 loss_train: 0.0622 acc_train: 0.9857 loss_val: 0.6066 acc_val: 0.8033 time: 0.0046s\n",
      "Epoch: 0112 loss_train: 0.0520 acc_train: 0.9857 loss_val: 0.6032 acc_val: 0.7967 time: 0.0044s\n",
      "Epoch: 0113 loss_train: 0.0435 acc_train: 0.9929 loss_val: 0.6014 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0114 loss_train: 0.0479 acc_train: 1.0000 loss_val: 0.5988 acc_val: 0.8000 time: 0.0165s\n",
      "Epoch: 0115 loss_train: 0.0371 acc_train: 1.0000 loss_val: 0.5967 acc_val: 0.8000 time: 0.0065s\n",
      "Epoch: 0116 loss_train: 0.0589 acc_train: 1.0000 loss_val: 0.5952 acc_val: 0.8000 time: 0.0074s\n",
      "Epoch: 0117 loss_train: 0.0524 acc_train: 0.9929 loss_val: 0.5960 acc_val: 0.8067 time: 0.0068s\n",
      "Epoch: 0118 loss_train: 0.0548 acc_train: 1.0000 loss_val: 0.5987 acc_val: 0.8067 time: 0.0069s\n",
      "Epoch: 0119 loss_train: 0.0497 acc_train: 0.9929 loss_val: 0.6022 acc_val: 0.8033 time: 0.0054s\n",
      "Epoch: 0120 loss_train: 0.0522 acc_train: 0.9929 loss_val: 0.6048 acc_val: 0.8033 time: 0.0076s\n",
      "Epoch: 0121 loss_train: 0.0467 acc_train: 0.9929 loss_val: 0.6096 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0122 loss_train: 0.0559 acc_train: 0.9929 loss_val: 0.6142 acc_val: 0.8000 time: 0.0059s\n",
      "Epoch: 0123 loss_train: 0.0691 acc_train: 0.9929 loss_val: 0.6160 acc_val: 0.7967 time: 0.0066s\n",
      "Epoch: 0124 loss_train: 0.0571 acc_train: 1.0000 loss_val: 0.6133 acc_val: 0.7967 time: 0.0079s\n",
      "Epoch: 0125 loss_train: 0.0589 acc_train: 0.9929 loss_val: 0.6100 acc_val: 0.7933 time: 0.0093s\n",
      "Epoch: 0126 loss_train: 0.0484 acc_train: 1.0000 loss_val: 0.6066 acc_val: 0.8000 time: 0.0061s\n",
      "Epoch: 0127 loss_train: 0.0462 acc_train: 0.9929 loss_val: 0.6046 acc_val: 0.7933 time: 0.0089s\n",
      "Epoch: 0128 loss_train: 0.0583 acc_train: 0.9929 loss_val: 0.6038 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0129 loss_train: 0.0817 acc_train: 0.9857 loss_val: 0.6036 acc_val: 0.8000 time: 0.0078s\n",
      "Epoch: 0130 loss_train: 0.0431 acc_train: 1.0000 loss_val: 0.6023 acc_val: 0.8033 time: 0.0078s\n",
      "Epoch: 0131 loss_train: 0.0488 acc_train: 0.9929 loss_val: 0.6002 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0132 loss_train: 0.0341 acc_train: 1.0000 loss_val: 0.5992 acc_val: 0.8133 time: 0.0079s\n",
      "Epoch: 0133 loss_train: 0.0463 acc_train: 1.0000 loss_val: 0.6002 acc_val: 0.8133 time: 0.0072s\n",
      "Epoch: 0134 loss_train: 0.0495 acc_train: 1.0000 loss_val: 0.6044 acc_val: 0.8000 time: 0.0061s\n",
      "Epoch: 0135 loss_train: 0.0499 acc_train: 0.9857 loss_val: 0.6088 acc_val: 0.8067 time: 0.0054s\n",
      "Epoch: 0136 loss_train: 0.0480 acc_train: 0.9857 loss_val: 0.6105 acc_val: 0.8033 time: 0.0058s\n",
      "Epoch: 0137 loss_train: 0.0486 acc_train: 0.9857 loss_val: 0.6133 acc_val: 0.8067 time: 0.0072s\n",
      "Epoch: 0138 loss_train: 0.0562 acc_train: 0.9857 loss_val: 0.6133 acc_val: 0.8000 time: 0.0058s\n",
      "Epoch: 0139 loss_train: 0.0280 acc_train: 1.0000 loss_val: 0.6127 acc_val: 0.8000 time: 0.0048s\n",
      "Epoch: 0140 loss_train: 0.0598 acc_train: 0.9929 loss_val: 0.6056 acc_val: 0.7933 time: 0.0057s\n",
      "Epoch: 0141 loss_train: 0.0345 acc_train: 1.0000 loss_val: 0.5980 acc_val: 0.7967 time: 0.0063s\n",
      "Epoch: 0142 loss_train: 0.0338 acc_train: 1.0000 loss_val: 0.5901 acc_val: 0.8000 time: 0.0059s\n",
      "Epoch: 0143 loss_train: 0.0348 acc_train: 1.0000 loss_val: 0.5851 acc_val: 0.8067 time: 0.0051s\n",
      "Epoch: 0144 loss_train: 0.0482 acc_train: 0.9929 loss_val: 0.5816 acc_val: 0.8000 time: 0.0054s\n",
      "Epoch: 0145 loss_train: 0.0359 acc_train: 1.0000 loss_val: 0.5794 acc_val: 0.8033 time: 0.0053s\n",
      "Epoch: 0146 loss_train: 0.0406 acc_train: 0.9929 loss_val: 0.5785 acc_val: 0.8067 time: 0.0055s\n",
      "Epoch: 0147 loss_train: 0.0687 acc_train: 0.9929 loss_val: 0.5752 acc_val: 0.8033 time: 0.0050s\n",
      "Epoch: 0148 loss_train: 0.0418 acc_train: 1.0000 loss_val: 0.5733 acc_val: 0.8067 time: 0.0047s\n",
      "Epoch: 0149 loss_train: 0.0368 acc_train: 0.9929 loss_val: 0.5740 acc_val: 0.8067 time: 0.0053s\n",
      "Epoch: 0150 loss_train: 0.0355 acc_train: 1.0000 loss_val: 0.5760 acc_val: 0.8100 time: 0.0062s\n",
      "Epoch: 0151 loss_train: 0.0429 acc_train: 1.0000 loss_val: 0.5786 acc_val: 0.8033 time: 0.0050s\n",
      "Epoch: 0152 loss_train: 0.0538 acc_train: 0.9929 loss_val: 0.5817 acc_val: 0.8067 time: 0.0058s\n",
      "Epoch: 0153 loss_train: 0.0453 acc_train: 0.9929 loss_val: 0.5863 acc_val: 0.8067 time: 0.0192s\n",
      "Epoch: 0154 loss_train: 0.0382 acc_train: 0.9929 loss_val: 0.5910 acc_val: 0.7933 time: 0.0061s\n",
      "Epoch: 0155 loss_train: 0.0411 acc_train: 1.0000 loss_val: 0.5956 acc_val: 0.8000 time: 0.0054s\n",
      "Epoch: 0156 loss_train: 0.0477 acc_train: 1.0000 loss_val: 0.6012 acc_val: 0.8000 time: 0.0049s\n",
      "Epoch: 0157 loss_train: 0.0325 acc_train: 1.0000 loss_val: 0.6055 acc_val: 0.8000 time: 0.0059s\n",
      "Epoch: 0158 loss_train: 0.0329 acc_train: 0.9929 loss_val: 0.6081 acc_val: 0.8033 time: 0.0056s\n",
      "Epoch: 0159 loss_train: 0.0404 acc_train: 0.9929 loss_val: 0.6078 acc_val: 0.8033 time: 0.0048s\n",
      "Epoch: 0160 loss_train: 0.0337 acc_train: 1.0000 loss_val: 0.6045 acc_val: 0.8033 time: 0.0059s\n",
      "Epoch: 0161 loss_train: 0.0463 acc_train: 0.9929 loss_val: 0.5999 acc_val: 0.8033 time: 0.0060s\n",
      "Epoch: 0162 loss_train: 0.0555 acc_train: 0.9929 loss_val: 0.5929 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0163 loss_train: 0.0423 acc_train: 0.9929 loss_val: 0.5845 acc_val: 0.8033 time: 0.0054s\n",
      "Epoch: 0164 loss_train: 0.0420 acc_train: 0.9929 loss_val: 0.5757 acc_val: 0.8167 time: 0.0049s\n",
      "Epoch: 0165 loss_train: 0.0391 acc_train: 1.0000 loss_val: 0.5703 acc_val: 0.8267 time: 0.0049s\n",
      "Epoch: 0166 loss_train: 0.0444 acc_train: 0.9929 loss_val: 0.5675 acc_val: 0.8300 time: 0.0049s\n",
      "Epoch: 0167 loss_train: 0.0445 acc_train: 0.9857 loss_val: 0.5650 acc_val: 0.8300 time: 0.0050s\n",
      "Epoch: 0168 loss_train: 0.0308 acc_train: 1.0000 loss_val: 0.5661 acc_val: 0.8267 time: 0.0045s\n",
      "Epoch: 0169 loss_train: 0.0440 acc_train: 0.9929 loss_val: 0.5689 acc_val: 0.8233 time: 0.0052s\n",
      "Epoch: 0170 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.5725 acc_val: 0.8133 time: 0.0045s\n",
      "Epoch: 0171 loss_train: 0.0559 acc_train: 0.9929 loss_val: 0.5773 acc_val: 0.8067 time: 0.0048s\n",
      "Epoch: 0172 loss_train: 0.0341 acc_train: 1.0000 loss_val: 0.5828 acc_val: 0.8033 time: 0.0049s\n",
      "Epoch: 0173 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.5877 acc_val: 0.8033 time: 0.0047s\n",
      "Epoch: 0174 loss_train: 0.0390 acc_train: 1.0000 loss_val: 0.5923 acc_val: 0.8067 time: 0.0052s\n",
      "Epoch: 0175 loss_train: 0.0339 acc_train: 0.9929 loss_val: 0.5998 acc_val: 0.8033 time: 0.0069s\n",
      "Epoch: 0176 loss_train: 0.0556 acc_train: 0.9857 loss_val: 0.6092 acc_val: 0.7933 time: 0.0185s\n",
      "Epoch: 0177 loss_train: 0.0349 acc_train: 1.0000 loss_val: 0.6162 acc_val: 0.7933 time: 0.0047s\n",
      "Epoch: 0178 loss_train: 0.0391 acc_train: 0.9929 loss_val: 0.6222 acc_val: 0.7933 time: 0.0044s\n",
      "Epoch: 0179 loss_train: 0.0486 acc_train: 0.9929 loss_val: 0.6277 acc_val: 0.7800 time: 0.0046s\n",
      "Epoch: 0180 loss_train: 0.0345 acc_train: 1.0000 loss_val: 0.6317 acc_val: 0.7800 time: 0.0049s\n",
      "Epoch: 0181 loss_train: 0.0307 acc_train: 1.0000 loss_val: 0.6320 acc_val: 0.7800 time: 0.0052s\n",
      "Epoch: 0182 loss_train: 0.0410 acc_train: 0.9929 loss_val: 0.6265 acc_val: 0.7833 time: 0.0045s\n",
      "Epoch: 0183 loss_train: 0.0453 acc_train: 0.9929 loss_val: 0.6177 acc_val: 0.7833 time: 0.0048s\n",
      "Epoch: 0184 loss_train: 0.0439 acc_train: 1.0000 loss_val: 0.6109 acc_val: 0.7833 time: 0.0048s\n",
      "Epoch: 0185 loss_train: 0.0649 acc_train: 0.9929 loss_val: 0.6043 acc_val: 0.7900 time: 0.0049s\n",
      "Epoch: 0186 loss_train: 0.0355 acc_train: 1.0000 loss_val: 0.5977 acc_val: 0.7967 time: 0.0050s\n",
      "Epoch: 0187 loss_train: 0.0481 acc_train: 1.0000 loss_val: 0.5910 acc_val: 0.7967 time: 0.0049s\n",
      "Epoch: 0188 loss_train: 0.0466 acc_train: 0.9929 loss_val: 0.5841 acc_val: 0.8000 time: 0.0048s\n",
      "Epoch: 0189 loss_train: 0.0317 acc_train: 1.0000 loss_val: 0.5786 acc_val: 0.8100 time: 0.0045s\n",
      "Epoch: 0190 loss_train: 0.0366 acc_train: 0.9929 loss_val: 0.5761 acc_val: 0.8167 time: 0.0048s\n",
      "Epoch: 0191 loss_train: 0.0399 acc_train: 1.0000 loss_val: 0.5738 acc_val: 0.8133 time: 0.0048s\n",
      "Epoch: 0192 loss_train: 0.0372 acc_train: 1.0000 loss_val: 0.5733 acc_val: 0.8133 time: 0.0048s\n",
      "Epoch: 0193 loss_train: 0.0270 acc_train: 1.0000 loss_val: 0.5734 acc_val: 0.8200 time: 0.0044s\n",
      "Epoch: 0194 loss_train: 0.0418 acc_train: 0.9929 loss_val: 0.5739 acc_val: 0.8200 time: 0.0048s\n",
      "Epoch: 0195 loss_train: 0.0660 acc_train: 0.9857 loss_val: 0.5773 acc_val: 0.8200 time: 0.0046s\n",
      "Epoch: 0196 loss_train: 0.0337 acc_train: 1.0000 loss_val: 0.5831 acc_val: 0.8133 time: 0.0046s\n",
      "Epoch: 0197 loss_train: 0.0525 acc_train: 1.0000 loss_val: 0.5920 acc_val: 0.8100 time: 0.0045s\n",
      "Epoch: 0198 loss_train: 0.0332 acc_train: 1.0000 loss_val: 0.5997 acc_val: 0.8100 time: 0.0052s\n",
      "Epoch: 0199 loss_train: 0.0442 acc_train: 0.9929 loss_val: 0.6050 acc_val: 0.7967 time: 0.0051s\n",
      "Epoch: 0200 loss_train: 0.0348 acc_train: 1.0000 loss_val: 0.6099 acc_val: 0.8067 time: 0.0054s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.2384s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(features, adj) \n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output_val = model(features, adj)\n",
    "    loss_val = F.nll_loss(output_val[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output_val[idx_val], labels[idx_val])\n",
    "\n",
    "    print(f'Epoch: {epoch+1:04d}',\n",
    "          f'loss_train: {loss_train.item():.4f}',\n",
    "          f'acc_train: {acc_train.item():.4f}',\n",
    "          f'loss_val: {loss_val.item():.4f}',\n",
    "          f'acc_val: {acc_val.item():.4f}',\n",
    "          f'time: {time.time() - t:.4f}s')\n",
    "    return loss_val.item()\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "t_total = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    train_epoch(epoch)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48146047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Test Set...\n",
      "\n",
      "Test set results: loss= 0.6196 accuracy= 0.8070\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"\\nTest set results:\",\n",
    "          f\"loss= {loss_test.item():.4f}\",\n",
    "          f\"accuracy= {acc_test.item():.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2796449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "189G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
