{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef38c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Dataset_Loader_Node_Classification import Dataset_Loader\n",
    "from gcn.models import GCN\n",
    "from gcn.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf84e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading citeseer dataset...\n",
      "Data Loaded:\n",
      "  Features shape: torch.Size([3312, 3703])\n",
      "  Adjacency matrix shape: torch.Size([3312, 3312])\n",
      "  Labels shape: torch.Size([3312])\n",
      "  Number of training samples: 120\n",
      "  Number of validation samples: 300\n",
      "  Number of testing samples: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atticus/atwong@ucdavis.edu/ECS189G/stage-5-189/_Citeseer/../Dataset_Loader_Node_Classification.py:35: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/torch/csrc/utils/tensor_new.cpp:653.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "data_loader = Dataset_Loader(dName='citeseer')\n",
    "data_loader.dataset_name = 'citeseer'\n",
    "data_loader.dataset_source_folder_path = '../stage_5_data/citeseer/'\n",
    "\n",
    "loaded_data = data_loader.load()\n",
    "\n",
    "graph_data = loaded_data['graph']\n",
    "train_test_val_indices = loaded_data['train_test_val']\n",
    "\n",
    "adj = graph_data['utility']['A']\n",
    "features = graph_data['X']\n",
    "labels = graph_data['y']\n",
    "\n",
    "idx_train = train_test_val_indices['idx_train']\n",
    "idx_val = train_test_val_indices['idx_val']\n",
    "idx_test = train_test_val_indices['idx_test']\n",
    "\n",
    "print(\"Data Loaded:\")\n",
    "print(f\"  Features shape: {features.shape}\")\n",
    "print(f\"  Adjacency matrix shape: {adj.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print(f\"  Number of training samples: {len(idx_train)}\")\n",
    "print(f\"  Number of validation samples: {len(idx_val)}\")\n",
    "print(f\"  Number of testing samples: {len(idx_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47b3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized:\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (3703 -> 16)\n",
      "  (gc2): GraphConvolution (16 -> 6)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "hidden_units = 16\n",
    "dropout_rate = 0.5\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "n_features = features.shape[1]\n",
    "n_classes = labels.max().item() + 1\n",
    "\n",
    "model = GCN(nfeat=n_features,\n",
    "            nhid=hidden_units,\n",
    "            nclass=n_classes,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda_available:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "print(\"Model Initialized:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b53e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch: 0001 loss_train: 1.9884 acc_train: 0.0417 loss_val: 1.7747 acc_val: 0.1967 time: 0.0530s\n",
      "Epoch: 0002 loss_train: 1.7510 acc_train: 0.1583 loss_val: 1.7782 acc_val: 0.1900 time: 0.0106s\n",
      "Epoch: 0003 loss_train: 1.6091 acc_train: 0.3417 loss_val: 1.7876 acc_val: 0.2533 time: 0.0087s\n",
      "Epoch: 0004 loss_train: 1.4304 acc_train: 0.6667 loss_val: 1.8072 acc_val: 0.2167 time: 0.0086s\n",
      "Epoch: 0005 loss_train: 1.3125 acc_train: 0.7083 loss_val: 1.8424 acc_val: 0.1900 time: 0.0085s\n",
      "Epoch: 0006 loss_train: 1.1806 acc_train: 0.7417 loss_val: 1.8947 acc_val: 0.1700 time: 0.0087s\n",
      "Epoch: 0007 loss_train: 1.0690 acc_train: 0.7833 loss_val: 1.9606 acc_val: 0.1500 time: 0.0090s\n",
      "Epoch: 0008 loss_train: 0.9432 acc_train: 0.7750 loss_val: 2.0202 acc_val: 0.1433 time: 0.0089s\n",
      "Epoch: 0009 loss_train: 0.8564 acc_train: 0.7750 loss_val: 2.0642 acc_val: 0.1533 time: 0.0088s\n",
      "Epoch: 0010 loss_train: 0.8193 acc_train: 0.7833 loss_val: 2.0787 acc_val: 0.1767 time: 0.0082s\n",
      "Epoch: 0011 loss_train: 0.7731 acc_train: 0.8167 loss_val: 2.0700 acc_val: 0.2133 time: 0.0091s\n",
      "Epoch: 0012 loss_train: 0.7114 acc_train: 0.8000 loss_val: 2.0434 acc_val: 0.2400 time: 0.0089s\n",
      "Epoch: 0013 loss_train: 0.6431 acc_train: 0.8417 loss_val: 2.0143 acc_val: 0.2867 time: 0.0104s\n",
      "Epoch: 0014 loss_train: 0.5993 acc_train: 0.8583 loss_val: 1.9867 acc_val: 0.3067 time: 0.0089s\n",
      "Epoch: 0015 loss_train: 0.5086 acc_train: 0.8417 loss_val: 1.9610 acc_val: 0.3200 time: 0.0091s\n",
      "Epoch: 0016 loss_train: 0.4822 acc_train: 0.8667 loss_val: 1.9344 acc_val: 0.3267 time: 0.0097s\n",
      "Epoch: 0017 loss_train: 0.4888 acc_train: 0.8583 loss_val: 1.9085 acc_val: 0.3400 time: 0.0095s\n",
      "Epoch: 0018 loss_train: 0.3571 acc_train: 0.9083 loss_val: 1.8780 acc_val: 0.3767 time: 0.0107s\n",
      "Epoch: 0019 loss_train: 0.3180 acc_train: 0.9417 loss_val: 1.8479 acc_val: 0.3867 time: 0.0126s\n",
      "Epoch: 0020 loss_train: 0.3154 acc_train: 0.9250 loss_val: 1.8172 acc_val: 0.3933 time: 0.0195s\n",
      "Epoch: 0021 loss_train: 0.3005 acc_train: 0.9000 loss_val: 1.7860 acc_val: 0.3933 time: 0.0102s\n",
      "Epoch: 0022 loss_train: 0.2734 acc_train: 0.9417 loss_val: 1.7564 acc_val: 0.3900 time: 0.0094s\n",
      "Epoch: 0023 loss_train: 0.1961 acc_train: 0.9250 loss_val: 1.7244 acc_val: 0.3967 time: 0.0093s\n",
      "Epoch: 0024 loss_train: 0.2637 acc_train: 0.9167 loss_val: 1.6947 acc_val: 0.4067 time: 0.0094s\n",
      "Epoch: 0025 loss_train: 0.1752 acc_train: 0.9583 loss_val: 1.6708 acc_val: 0.4000 time: 0.0086s\n",
      "Epoch: 0026 loss_train: 0.1914 acc_train: 0.9500 loss_val: 1.6482 acc_val: 0.4067 time: 0.0085s\n",
      "Epoch: 0027 loss_train: 0.1624 acc_train: 0.9750 loss_val: 1.6263 acc_val: 0.3967 time: 0.0085s\n",
      "Epoch: 0028 loss_train: 0.1592 acc_train: 0.9500 loss_val: 1.6062 acc_val: 0.4033 time: 0.0087s\n",
      "Epoch: 0029 loss_train: 0.1405 acc_train: 0.9750 loss_val: 1.5897 acc_val: 0.4233 time: 0.0087s\n",
      "Epoch: 0030 loss_train: 0.1988 acc_train: 0.9250 loss_val: 1.5759 acc_val: 0.4167 time: 0.0084s\n",
      "Epoch: 0031 loss_train: 0.1470 acc_train: 0.9667 loss_val: 1.5699 acc_val: 0.4067 time: 0.0088s\n",
      "Epoch: 0032 loss_train: 0.1229 acc_train: 0.9833 loss_val: 1.5630 acc_val: 0.4100 time: 0.0085s\n",
      "Epoch: 0033 loss_train: 0.0940 acc_train: 0.9917 loss_val: 1.5576 acc_val: 0.4167 time: 0.0092s\n",
      "Epoch: 0034 loss_train: 0.1094 acc_train: 0.9750 loss_val: 1.5535 acc_val: 0.4167 time: 0.0096s\n",
      "Epoch: 0035 loss_train: 0.1076 acc_train: 0.9833 loss_val: 1.5551 acc_val: 0.4200 time: 0.0118s\n",
      "Epoch: 0036 loss_train: 0.0669 acc_train: 1.0000 loss_val: 1.5591 acc_val: 0.4233 time: 0.0087s\n",
      "Epoch: 0037 loss_train: 0.0833 acc_train: 1.0000 loss_val: 1.5668 acc_val: 0.4200 time: 0.0085s\n",
      "Epoch: 0038 loss_train: 0.0749 acc_train: 0.9917 loss_val: 1.5703 acc_val: 0.4200 time: 0.0085s\n",
      "Epoch: 0039 loss_train: 0.0733 acc_train: 0.9917 loss_val: 1.5721 acc_val: 0.4267 time: 0.0089s\n",
      "Epoch: 0040 loss_train: 0.0782 acc_train: 0.9917 loss_val: 1.5846 acc_val: 0.4267 time: 0.0099s\n",
      "Epoch: 0041 loss_train: 0.0815 acc_train: 0.9833 loss_val: 1.5954 acc_val: 0.4233 time: 0.0088s\n",
      "Epoch: 0042 loss_train: 0.0629 acc_train: 0.9917 loss_val: 1.6006 acc_val: 0.4233 time: 0.0142s\n",
      "Epoch: 0043 loss_train: 0.0589 acc_train: 1.0000 loss_val: 1.6063 acc_val: 0.4233 time: 0.0106s\n",
      "Epoch: 0044 loss_train: 0.0527 acc_train: 0.9833 loss_val: 1.6120 acc_val: 0.4233 time: 0.0110s\n",
      "Epoch: 0045 loss_train: 0.0395 acc_train: 1.0000 loss_val: 1.6176 acc_val: 0.4133 time: 0.0089s\n",
      "Epoch: 0046 loss_train: 0.0510 acc_train: 0.9917 loss_val: 1.6182 acc_val: 0.4200 time: 0.0104s\n",
      "Epoch: 0047 loss_train: 0.0394 acc_train: 1.0000 loss_val: 1.6113 acc_val: 0.4233 time: 0.0235s\n",
      "Epoch: 0048 loss_train: 0.0470 acc_train: 0.9750 loss_val: 1.5930 acc_val: 0.4433 time: 0.0100s\n",
      "Epoch: 0049 loss_train: 0.0503 acc_train: 0.9917 loss_val: 1.5780 acc_val: 0.4533 time: 0.0087s\n",
      "Epoch: 0050 loss_train: 0.0465 acc_train: 0.9917 loss_val: 1.5671 acc_val: 0.4633 time: 0.0088s\n",
      "Epoch: 0051 loss_train: 0.0540 acc_train: 0.9917 loss_val: 1.5611 acc_val: 0.4600 time: 0.0132s\n",
      "Epoch: 0052 loss_train: 0.0612 acc_train: 0.9917 loss_val: 1.5590 acc_val: 0.4633 time: 0.0090s\n",
      "Epoch: 0053 loss_train: 0.0560 acc_train: 0.9917 loss_val: 1.5634 acc_val: 0.4600 time: 0.0089s\n",
      "Epoch: 0054 loss_train: 0.0457 acc_train: 0.9917 loss_val: 1.5733 acc_val: 0.4533 time: 0.0088s\n",
      "Epoch: 0055 loss_train: 0.0425 acc_train: 0.9833 loss_val: 1.5884 acc_val: 0.4467 time: 0.0089s\n",
      "Epoch: 0056 loss_train: 0.0526 acc_train: 0.9917 loss_val: 1.6045 acc_val: 0.4467 time: 0.0089s\n",
      "Epoch: 0057 loss_train: 0.0513 acc_train: 1.0000 loss_val: 1.6223 acc_val: 0.4433 time: 0.0090s\n",
      "Epoch: 0058 loss_train: 0.0482 acc_train: 1.0000 loss_val: 1.6368 acc_val: 0.4367 time: 0.0090s\n",
      "Epoch: 0059 loss_train: 0.0577 acc_train: 0.9833 loss_val: 1.6416 acc_val: 0.4367 time: 0.0090s\n",
      "Epoch: 0060 loss_train: 0.0461 acc_train: 0.9917 loss_val: 1.6548 acc_val: 0.4300 time: 0.0099s\n",
      "Epoch: 0061 loss_train: 0.0610 acc_train: 0.9833 loss_val: 1.6593 acc_val: 0.4300 time: 0.0093s\n",
      "Epoch: 0062 loss_train: 0.0639 acc_train: 0.9917 loss_val: 1.6528 acc_val: 0.4233 time: 0.0091s\n",
      "Epoch: 0063 loss_train: 0.0664 acc_train: 0.9833 loss_val: 1.6344 acc_val: 0.4433 time: 0.0088s\n",
      "Epoch: 0064 loss_train: 0.0665 acc_train: 0.9750 loss_val: 1.6211 acc_val: 0.4500 time: 0.0086s\n",
      "Epoch: 0065 loss_train: 0.0252 acc_train: 1.0000 loss_val: 1.6076 acc_val: 0.4567 time: 0.0088s\n",
      "Epoch: 0066 loss_train: 0.0338 acc_train: 1.0000 loss_val: 1.6014 acc_val: 0.4633 time: 0.0085s\n",
      "Epoch: 0067 loss_train: 0.0304 acc_train: 1.0000 loss_val: 1.5988 acc_val: 0.4667 time: 0.0086s\n",
      "Epoch: 0068 loss_train: 0.0505 acc_train: 1.0000 loss_val: 1.6003 acc_val: 0.4633 time: 0.0086s\n",
      "Epoch: 0069 loss_train: 0.0560 acc_train: 0.9833 loss_val: 1.6106 acc_val: 0.4467 time: 0.0086s\n",
      "Epoch: 0070 loss_train: 0.0412 acc_train: 1.0000 loss_val: 1.6250 acc_val: 0.4400 time: 0.0086s\n",
      "Epoch: 0071 loss_train: 0.0714 acc_train: 0.9750 loss_val: 1.6316 acc_val: 0.4500 time: 0.0087s\n",
      "Epoch: 0072 loss_train: 0.0503 acc_train: 0.9917 loss_val: 1.6299 acc_val: 0.4433 time: 0.0091s\n",
      "Epoch: 0073 loss_train: 0.0273 acc_train: 1.0000 loss_val: 1.6260 acc_val: 0.4500 time: 0.0092s\n",
      "Epoch: 0074 loss_train: 0.0489 acc_train: 1.0000 loss_val: 1.6110 acc_val: 0.4500 time: 0.0093s\n",
      "Epoch: 0075 loss_train: 0.0506 acc_train: 0.9917 loss_val: 1.5979 acc_val: 0.4633 time: 0.0088s\n",
      "Epoch: 0076 loss_train: 0.0297 acc_train: 1.0000 loss_val: 1.5902 acc_val: 0.4633 time: 0.0087s\n",
      "Epoch: 0077 loss_train: 0.0327 acc_train: 1.0000 loss_val: 1.5828 acc_val: 0.4667 time: 0.0087s\n",
      "Epoch: 0078 loss_train: 0.0390 acc_train: 1.0000 loss_val: 1.5749 acc_val: 0.4733 time: 0.0087s\n",
      "Epoch: 0079 loss_train: 0.0388 acc_train: 1.0000 loss_val: 1.5685 acc_val: 0.4767 time: 0.0086s\n",
      "Epoch: 0080 loss_train: 0.0300 acc_train: 1.0000 loss_val: 1.5597 acc_val: 0.4833 time: 0.0087s\n",
      "Epoch: 0081 loss_train: 0.0480 acc_train: 0.9833 loss_val: 1.5527 acc_val: 0.4767 time: 0.0086s\n",
      "Epoch: 0082 loss_train: 0.0469 acc_train: 0.9917 loss_val: 1.5409 acc_val: 0.4733 time: 0.0090s\n",
      "Epoch: 0083 loss_train: 0.0386 acc_train: 1.0000 loss_val: 1.5388 acc_val: 0.4700 time: 0.0100s\n",
      "Epoch: 0084 loss_train: 0.0298 acc_train: 1.0000 loss_val: 1.5423 acc_val: 0.4733 time: 0.0087s\n",
      "Epoch: 0085 loss_train: 0.0388 acc_train: 0.9917 loss_val: 1.5473 acc_val: 0.4767 time: 0.0087s\n",
      "Epoch: 0086 loss_train: 0.0397 acc_train: 1.0000 loss_val: 1.5491 acc_val: 0.4767 time: 0.0087s\n",
      "Epoch: 0087 loss_train: 0.0393 acc_train: 0.9917 loss_val: 1.5482 acc_val: 0.4700 time: 0.0087s\n",
      "Epoch: 0088 loss_train: 0.0350 acc_train: 0.9917 loss_val: 1.5382 acc_val: 0.4733 time: 0.0085s\n",
      "Epoch: 0089 loss_train: 0.0469 acc_train: 0.9917 loss_val: 1.5270 acc_val: 0.4767 time: 0.0087s\n",
      "Epoch: 0090 loss_train: 0.0635 acc_train: 0.9917 loss_val: 1.5084 acc_val: 0.4800 time: 0.0088s\n",
      "Epoch: 0091 loss_train: 0.0507 acc_train: 1.0000 loss_val: 1.5022 acc_val: 0.4833 time: 0.0086s\n",
      "Epoch: 0092 loss_train: 0.0691 acc_train: 0.9667 loss_val: 1.4948 acc_val: 0.4933 time: 0.0086s\n",
      "Epoch: 0093 loss_train: 0.0520 acc_train: 0.9917 loss_val: 1.4947 acc_val: 0.4900 time: 0.0086s\n",
      "Epoch: 0094 loss_train: 0.0341 acc_train: 1.0000 loss_val: 1.5007 acc_val: 0.4833 time: 0.0089s\n",
      "Epoch: 0095 loss_train: 0.0274 acc_train: 1.0000 loss_val: 1.5142 acc_val: 0.4900 time: 0.0086s\n",
      "Epoch: 0096 loss_train: 0.0473 acc_train: 0.9917 loss_val: 1.5330 acc_val: 0.4867 time: 0.0087s\n",
      "Epoch: 0097 loss_train: 0.0248 acc_train: 1.0000 loss_val: 1.5511 acc_val: 0.4833 time: 0.0086s\n",
      "Epoch: 0098 loss_train: 0.0225 acc_train: 1.0000 loss_val: 1.5756 acc_val: 0.4800 time: 0.0088s\n",
      "Epoch: 0099 loss_train: 0.0316 acc_train: 1.0000 loss_val: 1.5985 acc_val: 0.4733 time: 0.0088s\n",
      "Epoch: 0100 loss_train: 0.0417 acc_train: 0.9917 loss_val: 1.6037 acc_val: 0.4700 time: 0.0084s\n",
      "Epoch: 0101 loss_train: 0.0305 acc_train: 1.0000 loss_val: 1.6094 acc_val: 0.4767 time: 0.0086s\n",
      "Epoch: 0102 loss_train: 0.0601 acc_train: 0.9750 loss_val: 1.6024 acc_val: 0.4733 time: 0.0087s\n",
      "Epoch: 0103 loss_train: 0.0330 acc_train: 1.0000 loss_val: 1.5981 acc_val: 0.4700 time: 0.0085s\n",
      "Epoch: 0104 loss_train: 0.0304 acc_train: 1.0000 loss_val: 1.5874 acc_val: 0.4800 time: 0.0089s\n",
      "Epoch: 0105 loss_train: 0.0443 acc_train: 0.9917 loss_val: 1.5679 acc_val: 0.4800 time: 0.0100s\n",
      "Epoch: 0106 loss_train: 0.0361 acc_train: 1.0000 loss_val: 1.5519 acc_val: 0.4867 time: 0.0095s\n",
      "Epoch: 0107 loss_train: 0.0465 acc_train: 0.9833 loss_val: 1.5452 acc_val: 0.4867 time: 0.0089s\n",
      "Epoch: 0108 loss_train: 0.0471 acc_train: 0.9917 loss_val: 1.5421 acc_val: 0.4833 time: 0.0088s\n",
      "Epoch: 0109 loss_train: 0.0521 acc_train: 0.9917 loss_val: 1.5297 acc_val: 0.4867 time: 0.0088s\n",
      "Epoch: 0110 loss_train: 0.0347 acc_train: 1.0000 loss_val: 1.5319 acc_val: 0.4867 time: 0.0086s\n",
      "Epoch: 0111 loss_train: 0.0404 acc_train: 1.0000 loss_val: 1.5450 acc_val: 0.4900 time: 0.0085s\n",
      "Epoch: 0112 loss_train: 0.0271 acc_train: 1.0000 loss_val: 1.5639 acc_val: 0.4867 time: 0.0085s\n",
      "Epoch: 0113 loss_train: 0.0192 acc_train: 1.0000 loss_val: 1.5847 acc_val: 0.4800 time: 0.0087s\n",
      "Epoch: 0114 loss_train: 0.0319 acc_train: 1.0000 loss_val: 1.5978 acc_val: 0.4833 time: 0.0087s\n",
      "Epoch: 0115 loss_train: 0.0350 acc_train: 0.9917 loss_val: 1.6186 acc_val: 0.4833 time: 0.0087s\n",
      "Epoch: 0116 loss_train: 0.0272 acc_train: 1.0000 loss_val: 1.6307 acc_val: 0.4833 time: 0.0087s\n",
      "Epoch: 0117 loss_train: 0.0286 acc_train: 0.9917 loss_val: 1.6294 acc_val: 0.4833 time: 0.0090s\n",
      "Epoch: 0118 loss_train: 0.0445 acc_train: 0.9917 loss_val: 1.6220 acc_val: 0.4833 time: 0.0087s\n",
      "Epoch: 0119 loss_train: 0.0316 acc_train: 1.0000 loss_val: 1.6049 acc_val: 0.4867 time: 0.0087s\n",
      "Epoch: 0120 loss_train: 0.0323 acc_train: 1.0000 loss_val: 1.5879 acc_val: 0.4867 time: 0.0084s\n",
      "Epoch: 0121 loss_train: 0.0270 acc_train: 1.0000 loss_val: 1.5782 acc_val: 0.4900 time: 0.0090s\n",
      "Epoch: 0122 loss_train: 0.0301 acc_train: 1.0000 loss_val: 1.5798 acc_val: 0.4800 time: 0.0091s\n",
      "Epoch: 0123 loss_train: 0.0412 acc_train: 0.9917 loss_val: 1.5967 acc_val: 0.4733 time: 0.0083s\n",
      "Epoch: 0124 loss_train: 0.0419 acc_train: 1.0000 loss_val: 1.6000 acc_val: 0.4767 time: 0.0086s\n",
      "Epoch: 0125 loss_train: 0.0281 acc_train: 1.0000 loss_val: 1.6055 acc_val: 0.4900 time: 0.0086s\n",
      "Epoch: 0126 loss_train: 0.0396 acc_train: 0.9917 loss_val: 1.6211 acc_val: 0.4867 time: 0.0085s\n",
      "Epoch: 0127 loss_train: 0.0272 acc_train: 1.0000 loss_val: 1.6367 acc_val: 0.4800 time: 0.0101s\n",
      "Epoch: 0128 loss_train: 0.0505 acc_train: 0.9833 loss_val: 1.6467 acc_val: 0.4833 time: 0.0097s\n",
      "Epoch: 0129 loss_train: 0.0405 acc_train: 0.9917 loss_val: 1.6445 acc_val: 0.4833 time: 0.0138s\n",
      "Epoch: 0130 loss_train: 0.0490 acc_train: 0.9833 loss_val: 1.6355 acc_val: 0.4867 time: 0.0088s\n",
      "Epoch: 0131 loss_train: 0.0309 acc_train: 1.0000 loss_val: 1.6278 acc_val: 0.4900 time: 0.0089s\n",
      "Epoch: 0132 loss_train: 0.0235 acc_train: 0.9917 loss_val: 1.6245 acc_val: 0.4900 time: 0.0087s\n",
      "Epoch: 0133 loss_train: 0.0293 acc_train: 1.0000 loss_val: 1.6175 acc_val: 0.4900 time: 0.0086s\n",
      "Epoch: 0134 loss_train: 0.0421 acc_train: 1.0000 loss_val: 1.6111 acc_val: 0.4900 time: 0.0091s\n",
      "Epoch: 0135 loss_train: 0.0246 acc_train: 0.9917 loss_val: 1.5787 acc_val: 0.4933 time: 0.0088s\n",
      "Epoch: 0136 loss_train: 0.0507 acc_train: 0.9833 loss_val: 1.5550 acc_val: 0.5000 time: 0.0086s\n",
      "Epoch: 0137 loss_train: 0.0311 acc_train: 0.9917 loss_val: 1.5372 acc_val: 0.5000 time: 0.0084s\n",
      "Epoch: 0138 loss_train: 0.0669 acc_train: 0.9833 loss_val: 1.5226 acc_val: 0.5033 time: 0.0101s\n",
      "Epoch: 0139 loss_train: 0.0251 acc_train: 1.0000 loss_val: 1.5192 acc_val: 0.5000 time: 0.0090s\n",
      "Epoch: 0140 loss_train: 0.0228 acc_train: 1.0000 loss_val: 1.5216 acc_val: 0.5000 time: 0.0087s\n",
      "Epoch: 0141 loss_train: 0.0476 acc_train: 0.9917 loss_val: 1.5297 acc_val: 0.4933 time: 0.0094s\n",
      "Epoch: 0142 loss_train: 0.0277 acc_train: 1.0000 loss_val: 1.5408 acc_val: 0.4900 time: 0.0088s\n",
      "Epoch: 0143 loss_train: 0.0505 acc_train: 0.9833 loss_val: 1.5521 acc_val: 0.4900 time: 0.0087s\n",
      "Epoch: 0144 loss_train: 0.0483 acc_train: 1.0000 loss_val: 1.5710 acc_val: 0.4900 time: 0.0085s\n",
      "Epoch: 0145 loss_train: 0.0225 acc_train: 1.0000 loss_val: 1.5959 acc_val: 0.4967 time: 0.0083s\n",
      "Epoch: 0146 loss_train: 0.0269 acc_train: 1.0000 loss_val: 1.6127 acc_val: 0.4933 time: 0.0085s\n",
      "Epoch: 0147 loss_train: 0.0312 acc_train: 0.9917 loss_val: 1.6266 acc_val: 0.4867 time: 0.0088s\n",
      "Epoch: 0148 loss_train: 0.0365 acc_train: 0.9917 loss_val: 1.6474 acc_val: 0.4800 time: 0.0085s\n",
      "Epoch: 0149 loss_train: 0.0159 acc_train: 1.0000 loss_val: 1.6669 acc_val: 0.4800 time: 0.0089s\n",
      "Epoch: 0150 loss_train: 0.0229 acc_train: 1.0000 loss_val: 1.6750 acc_val: 0.4767 time: 0.0092s\n",
      "Epoch: 0151 loss_train: 0.0272 acc_train: 1.0000 loss_val: 1.6761 acc_val: 0.4767 time: 0.0094s\n",
      "Epoch: 0152 loss_train: 0.0337 acc_train: 1.0000 loss_val: 1.6631 acc_val: 0.4833 time: 0.0092s\n",
      "Epoch: 0153 loss_train: 0.0281 acc_train: 1.0000 loss_val: 1.6402 acc_val: 0.4933 time: 0.0088s\n",
      "Epoch: 0154 loss_train: 0.0367 acc_train: 0.9917 loss_val: 1.6254 acc_val: 0.4933 time: 0.0086s\n",
      "Epoch: 0155 loss_train: 0.0249 acc_train: 1.0000 loss_val: 1.6130 acc_val: 0.4967 time: 0.0085s\n",
      "Epoch: 0156 loss_train: 0.0402 acc_train: 1.0000 loss_val: 1.6052 acc_val: 0.5067 time: 0.0088s\n",
      "Epoch: 0157 loss_train: 0.0219 acc_train: 1.0000 loss_val: 1.6052 acc_val: 0.5033 time: 0.0087s\n",
      "Epoch: 0158 loss_train: 0.0255 acc_train: 0.9917 loss_val: 1.6062 acc_val: 0.5100 time: 0.0087s\n",
      "Epoch: 0159 loss_train: 0.0436 acc_train: 1.0000 loss_val: 1.6234 acc_val: 0.4900 time: 0.0086s\n",
      "Epoch: 0160 loss_train: 0.0369 acc_train: 1.0000 loss_val: 1.6362 acc_val: 0.4767 time: 0.0086s\n",
      "Epoch: 0161 loss_train: 0.0310 acc_train: 1.0000 loss_val: 1.6508 acc_val: 0.4733 time: 0.0089s\n",
      "Epoch: 0162 loss_train: 0.0302 acc_train: 1.0000 loss_val: 1.6599 acc_val: 0.4700 time: 0.0088s\n",
      "Epoch: 0163 loss_train: 0.0325 acc_train: 0.9917 loss_val: 1.6366 acc_val: 0.4700 time: 0.0092s\n",
      "Epoch: 0164 loss_train: 0.0183 acc_train: 1.0000 loss_val: 1.6128 acc_val: 0.4733 time: 0.0221s\n",
      "Epoch: 0165 loss_train: 0.0340 acc_train: 0.9917 loss_val: 1.5930 acc_val: 0.4833 time: 0.0088s\n",
      "Epoch: 0166 loss_train: 0.0327 acc_train: 0.9917 loss_val: 1.5810 acc_val: 0.4933 time: 0.0086s\n",
      "Epoch: 0167 loss_train: 0.0321 acc_train: 0.9917 loss_val: 1.5531 acc_val: 0.5000 time: 0.0085s\n",
      "Epoch: 0168 loss_train: 0.0223 acc_train: 1.0000 loss_val: 1.5361 acc_val: 0.5133 time: 0.0084s\n",
      "Epoch: 0169 loss_train: 0.0335 acc_train: 0.9917 loss_val: 1.5348 acc_val: 0.5133 time: 0.0090s\n",
      "Epoch: 0170 loss_train: 0.0332 acc_train: 0.9917 loss_val: 1.5307 acc_val: 0.5167 time: 0.0090s\n",
      "Epoch: 0171 loss_train: 0.0360 acc_train: 0.9917 loss_val: 1.5323 acc_val: 0.5200 time: 0.0095s\n",
      "Epoch: 0172 loss_train: 0.0435 acc_train: 0.9917 loss_val: 1.5430 acc_val: 0.5067 time: 0.0093s\n",
      "Epoch: 0173 loss_train: 0.0288 acc_train: 1.0000 loss_val: 1.5585 acc_val: 0.5067 time: 0.0094s\n",
      "Epoch: 0174 loss_train: 0.0432 acc_train: 0.9833 loss_val: 1.5499 acc_val: 0.5033 time: 0.0088s\n",
      "Epoch: 0175 loss_train: 0.0315 acc_train: 1.0000 loss_val: 1.5413 acc_val: 0.5033 time: 0.0088s\n",
      "Epoch: 0176 loss_train: 0.0403 acc_train: 0.9917 loss_val: 1.5294 acc_val: 0.5100 time: 0.0088s\n",
      "Epoch: 0177 loss_train: 0.0237 acc_train: 1.0000 loss_val: 1.5158 acc_val: 0.5133 time: 0.0085s\n",
      "Epoch: 0178 loss_train: 0.0350 acc_train: 1.0000 loss_val: 1.5057 acc_val: 0.5067 time: 0.0086s\n",
      "Epoch: 0179 loss_train: 0.0418 acc_train: 0.9917 loss_val: 1.5025 acc_val: 0.5133 time: 0.0086s\n",
      "Epoch: 0180 loss_train: 0.0227 acc_train: 1.0000 loss_val: 1.5053 acc_val: 0.5100 time: 0.0100s\n",
      "Epoch: 0181 loss_train: 0.0170 acc_train: 1.0000 loss_val: 1.5139 acc_val: 0.5167 time: 0.0085s\n",
      "Epoch: 0182 loss_train: 0.0327 acc_train: 0.9917 loss_val: 1.5249 acc_val: 0.5133 time: 0.0087s\n",
      "Epoch: 0183 loss_train: 0.0216 acc_train: 1.0000 loss_val: 1.5365 acc_val: 0.5067 time: 0.0091s\n",
      "Epoch: 0184 loss_train: 0.0174 acc_train: 1.0000 loss_val: 1.5411 acc_val: 0.5100 time: 0.0094s\n",
      "Epoch: 0185 loss_train: 0.0142 acc_train: 1.0000 loss_val: 1.5490 acc_val: 0.5067 time: 0.0086s\n",
      "Epoch: 0186 loss_train: 0.0169 acc_train: 1.0000 loss_val: 1.5489 acc_val: 0.5167 time: 0.0087s\n",
      "Epoch: 0187 loss_train: 0.0203 acc_train: 1.0000 loss_val: 1.5499 acc_val: 0.5133 time: 0.0087s\n",
      "Epoch: 0188 loss_train: 0.0513 acc_train: 0.9833 loss_val: 1.5483 acc_val: 0.5167 time: 0.0086s\n",
      "Epoch: 0189 loss_train: 0.0323 acc_train: 1.0000 loss_val: 1.5470 acc_val: 0.5133 time: 0.0086s\n",
      "Epoch: 0190 loss_train: 0.0198 acc_train: 1.0000 loss_val: 1.5463 acc_val: 0.5067 time: 0.0085s\n",
      "Epoch: 0191 loss_train: 0.0249 acc_train: 1.0000 loss_val: 1.5465 acc_val: 0.5067 time: 0.0087s\n",
      "Epoch: 0192 loss_train: 0.0230 acc_train: 1.0000 loss_val: 1.5359 acc_val: 0.5100 time: 0.0084s\n",
      "Epoch: 0193 loss_train: 0.0312 acc_train: 0.9917 loss_val: 1.5269 acc_val: 0.5100 time: 0.0090s\n",
      "Epoch: 0194 loss_train: 0.0223 acc_train: 1.0000 loss_val: 1.5111 acc_val: 0.5133 time: 0.0086s\n",
      "Epoch: 0195 loss_train: 0.0358 acc_train: 0.9917 loss_val: 1.4757 acc_val: 0.5267 time: 0.0096s\n",
      "Epoch: 0196 loss_train: 0.0187 acc_train: 1.0000 loss_val: 1.4464 acc_val: 0.5267 time: 0.0089s\n",
      "Epoch: 0197 loss_train: 0.0297 acc_train: 0.9917 loss_val: 1.4697 acc_val: 0.5267 time: 0.0144s\n",
      "Epoch: 0198 loss_train: 0.0191 acc_train: 1.0000 loss_val: 1.4837 acc_val: 0.5233 time: 0.0085s\n",
      "Epoch: 0199 loss_train: 0.0353 acc_train: 1.0000 loss_val: 1.5089 acc_val: 0.5167 time: 0.0087s\n",
      "Epoch: 0200 loss_train: 0.0222 acc_train: 0.9917 loss_val: 1.5282 acc_val: 0.5100 time: 0.0087s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.9121s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(features, adj) \n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output_val = model(features, adj)\n",
    "    loss_val = F.nll_loss(output_val[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output_val[idx_val], labels[idx_val])\n",
    "\n",
    "    print(f'Epoch: {epoch+1:04d}',\n",
    "          f'loss_train: {loss_train.item():.4f}',\n",
    "          f'acc_train: {acc_train.item():.4f}',\n",
    "          f'loss_val: {loss_val.item():.4f}',\n",
    "          f'acc_val: {acc_val.item():.4f}',\n",
    "          f'time: {time.time() - t:.4f}s')\n",
    "    return loss_val.item()\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "t_total = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    train_epoch(epoch)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca6b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Test Set...\n",
      "\n",
      "Test set results: loss= 1.3965 accuracy= 0.6860\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"\\nTest set results:\",\n",
    "          f\"loss= {loss_test.item():.4f}\",\n",
    "          f\"accuracy= {acc_test.item():.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed22202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be1db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "189G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
